defaults:
  # Arguments dict passed to the env creator as an `EnvContext` object
  - _self_

# The environment specifier. This can either be a tune-registered env, via
# `tune.register_env([name], lambda env_ctx: [env object])`, or a string specifier of an
# RLlib supported type. In the latter case, RLlib will try to interpret the specifier as
# either an Farama-Foundation gymnasium env, a PyBullet env, a ViZDoomGym env, or a fully
# qualified classpath to an Env class, ie. "ray.rllib.examples.env.random_env.RandomEnv".
env: simharness2.environments.ReactiveHarness

# env_task_fn: null
# If True, try to render the environment on the local worker or on worker 1 (if
# num_rollout_workers > 0). For vectorized envs, this usually means that only the first
# sub-environment will be rendered. In order for this to work, your env will have to
# implement the `render()` method which either:
# a) handles window generation and rendering itself (returning True), or
# b) returns a numpy uint8 image of shape [height x width x 3 (RGB)]
render_env: true
# Whether to clip rewards during Policyâ€™s postprocessing. Options are:
# - None (default): Clip for Atari only (r=sign(r)).
# - True: r=sign(r): Fixed rewards -1.0, 1.0, or 0.0.
# - False: Never clip.
# - [float value]: Clip at -value and + value.
# - Tuple[value1, value2]: Clip at value1 and value2.
clip_rewards: null
# If True, RLlib will learn entirely inside a normalized action space (0.0 centered with
# small stddev; only affecting Box components). We will unsquash actions (and clip, just
# in case) to the bounds of the env's action space before sending actions back to env.
normalize_actions: true
disable_env_checking: false
is_atari: false
env_config:
  in_evaluation: false
  sim_init_cfg:
    simfire_cls:
      _target_: hydra.utils.get_class
      path: simfire.sim.simulation.FireSimulation
    config_dict: ${simulation.simfire}
  movements: [up, down, left, right]
  interactions: [fireline]
  attributes: [fire_map, elevation, w_0, sigma, delta, M_x]
  normalized_attributes: [elevation]
  agent_speed: 4
  # Defines the class that will be used to monitor and track `ReactiveHarness`.
  harness_analytics_partial:
    _target_: simharness2.analytics.harness_analytics.ReactiveHarnessAnalytics
    _partial_: true
    # Defines the class that will be used to monitor and track `FireSimulation`.
    sim_analytics_partial:
      _target_: simharness2.analytics.simulation_analytics.FireSimulationAnalytics
      _partial_: true
      # Defines the class that will be used to monitor and track agent behavior.
      agent_analytics_partial:
        _target_: simharness2.analytics.agent_analytics.ReactiveAgentAnalytics
        _partial_: true
        movement_types: ${....movements}
        interaction_types: ${....interactions}
  reward_init_cfg:
    reward_cls:
      _target_: hydra.utils.get_class
      path: simharness2.rewards.base_reward.SimpleReward
    kwargs: null
      # fixed_reward: 100
      # static_penalty: 50
      # invalid_movement_penalty: 5
  agent_initialization_cls:
    _target_: hydra.utils.get_class
    path: simharness2.agents.FixedPositionAgentInitializer
  agent_initialization_kwargs:
    agent_pos: [[0, 64]]
  action_space_cls:
    _target_: hydra.utils.get_class
    path: gymnasium.spaces.Discrete
