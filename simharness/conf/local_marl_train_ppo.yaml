defaults:
  - simulation: default
  - framework: default
  - rollouts: default
  - resources: default
  - hydra: default
  - evaluation: default
  - exploration: stochastic_sampling
  - training: ppo_with_custom_model
  - environment: marl_complex_obs # marl_damage_aware
  - _self_

# EX: Overriding environment config values.
# reward_mix_coeff: ??
# environment:
#   env_config:
#     # Defines the class that will be used to perform reward calculation at each timestep.
#     reward_init_cfg:
#       reward_cls:
#         _target_: hydra.utils.get_class
#         path: simharness2.rewards.base_reward.SimpleReward
#       kwargs:
#         mixing_coefficient: ${reward_mix_coeff}

cli:
  # Specify the run mode. Supported options: train, tune, view
  mode: train
  # Specify the root directory used to save data for the experiment.
  data_dir: ??

algo:
  name: PPO
  # Specify list of custom rllib callbacks to use during training.
  callbacks:
    - _target_: hydra.utils.get_class
      path: simharness2.callbacks.InitializeSimfire
    - _target_: hydra.utils.get_class
      path: simharness2.callbacks.RenderEnv
    - _target_: hydra.utils.get_class
      path: simharness2.callbacks.LandSavedMetric
  checkpoint_path: null

rollouts:
  # NOTE: MultiAgentComplexObsHarness DOES NOT normalize returned observations!
  # So, use the MeanStdFilter to ensure observations are normalized.
  # See the MeanStdObservationFilterCAgentConnector class for more:
  # https://github.com/ray-project/ray/blob/ad48682b4ec78a5699ba89a7c8a69327c264e47b/rllib/connectors/agent/mean_std_filter.py#L23
  enable_connectors: true
  observation_filter: MeanStdFilter
  batch_mode: complete_episodes # truncate_episodes
  # Scale experiment as desired
  # For `InitializeSimfire`, ensure correct env count (num_ws * num_envs_per_w) is used.
  # See bottom of file for the `InitializeSimfire` configuration!
  num_rollout_workers: 0
  num_envs_per_worker: 4

evaluation:
  # Adjust interval as needed!!
  evaluation_interval: 1
  evaluation_duration: ${evaluation.evaluation_num_workers}
  # For `InitializeSimfire`, ensure correct worker count is used.
  # See bottom of file for the `InitializeSimfire` configuration!
  evaluation_num_workers: 1

checkpoint:
  # Frequency at which to save checkpoints (in terms of training iterations)
  checkpoint_frequency: ${evaluation.evaluation_interval}

stop_conditions:
  training_iteration: 10000
  timesteps_total: 2000000000
  episodes_total: 1000000
  episode_reward_mean: 10000000

# Assume local training, so no GPUs avaialble. This can be overriden via CLI.
resources:
  num_gpus: 0
  num_gpus_per_worker: 0

debugging:
  log_level: INFO
  log_sys_usage: True
  seed: 2000
  logger_config:
    type:
      _target_: hydra.utils.get_class
      path: ray.tune.logger.UnifiedLogger
    logdir: ${hydra:run.dir}


# Total number of fire scenarios is:
# - For train, op_loc.sample_size.train * fire_initial_position.sampler.sample_size.train
# - For eval, op_loc.sample_size.eval * fire_initial_position.sampler.sample_size.eval
# - Op locs are fixed for the duration of the experiment (Open PR will fix this).
simulation:
  screen_size: 128
  # Configure the sampling of lon, lat coords used for operational location (s).
  operational_locations:
    # Path to the flattened BurnMD dataset.
    burnmd_dataset_path: ${oc.env:SH_HOME}/data/burnmd_TL_flat.json
    resample_interval: -1
    sample_size:
      train: 2
      eval: 1
    independent_eval: true
    # Number of operational locations from BurnMD that are used to create a population
    # of locations that will be sampled from every `resample_interval` episodes.
    # NOTE: This allows the user to control the total number of distinct operational
    # locations that will be used to produce sample batches (or trajectories) of 
    # experiences. If set to `None`, the entire BurnMD dataset will be used.
    # - Applies to training ONLY; eval "dataset" size is equal to `sample_size.eval`.
    # NOTE: Assume samples are selected randomly from the BurnMD dataset (using the
    # seed, if provided).
    population_size: 6
    seed: ${debugging.seed}
  fire_initial_position:
    # Configure the generation of candidate initial fire positions.
    generator:
      # If true, then all possible fire initial positions will be included in the
      # generated "dataset".
      make_all_positions: false
      # Number of candidate initial fire positions to generate. Ignored when
      # `make_all_postions == true`.
      output_size: 16
      # The root location to save the generated dataset.
      save_path: ${cli.data_dir}/simfire/data/fire_initial_positions
    # To disable usage of generator, simply set to null.
    # Configure the sampling of new initial fire positions.
    sampler:
      # Get (new) random sample of size `sample_size` every `resample_interval` episodes.
      # NOTE: Evaluation scenarios are never resampled; this applies to training ONLY.
      resample_interval: 1
      # Number of samples to draw from the distribution, ie. take `sample_size` initial
      # fire positions and distribute them across the respective RolloutWorker's.
      sample_size:
        train: 2
        eval: 1
      # If false, then eval scenarios are drawn from the same distribution as
      # training scenarios. This behavior is usually not desired, but can be
      # useful for debugging purposes, ie. seeing how well the trained policy
      # performs on a scenario from the training distribution.
      independent_eval: true
      # Filter generated "dataset" using the following condition.
      # NOTE: This is applied to the generated dataset, not the sampled positions.
      # For more information on the query string to evaluate, see:
      # - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html#pandas.DataFrame.query
      # Examples:
      # - "A < elapsed_steps", "elapsed_steps < B", "A < elapsed_steps < B"
      # - "(A < elapsed_steps) & (C < percent_area_burned)"
      # query: "50 < elapsed_steps < 150"
      # query: "0.10 < percent_area_burned" # FIXME: usign easy query to test
      query: "0.05 < percent_area_burned" # FIXME!
      # Number of samples available in resulting "dataset" after filtering with `query`.
      # NOTE: This allows the user to control the total number of distinct fire positions
      # that will be used to produce sample batches (or trajectories) of experiences. If
      # set to `None`, the population size will be equal to the size of the generated
      # "dataset" after filtering with `query`.
      # - Applies to training ONLY; eval "dataset" size is equal to `sample_size.eval`.
      population_size: 8 # FIXME!
      seed: ${debugging.seed}
