# Number of rollout worker actors to create for parallel sampling. Setting this to 0 will
# force rollouts to be done in the local worker (driver process or the Algorithm’s actor
# when using Tune).
num_env_runners: 8
# Number of environments to evaluate vector-wise per worker. This enables model inference
# batching, which can improve performance for inference bottlenecked workloads.
num_envs_per_env_runner: 1

# The `SampleCollector` class to be used to collect and retrieve environment-, model-,
# and sampler data. Override the `SampleCollector` base class to implement your own
# collection/buffering/retrieval logic.
# sample_collector: null

# Divide episodes into fragments of this many steps each during rollouts. Trajectories
# of this size are collected from rollout workers and combined into a larger batch of
# `train_batch_size` for learning. For example, given rollout_fragment_length=100 and
# train_batch_size=1000:
#   1. RLlib collects 10 fragments of 100 steps each from rollout workers.
#   2. These fragments are concatenated and we perform an epoch of SGD.
# When using multiple envs per worker, the fragment size is multiplied by
# `num_envs_per_worker`. This is since we are collecting steps from multiple envs in
# parallel. For example, if num_envs_per_worker=5, then rollout workers will return
# experiences in chunks of 5*100 = 500 steps. The dataflow here can vary per algorithm.
# For example, PPO further divides the train batch into minibatches for multi-epoch SGD.
# Set to “auto” to have RLlib compute an exact `rollout_fragment_length` to match the
# given batch size.
rollout_fragment_length: "auto"

# How to build per-Sampler (RolloutWorker) batches, which are then usually concat’d to
# form the train batch. Note that “steps” below can mean different things (either env-
# or agent-steps) and depends on the `count_steps_by` setting:
# 1. “truncate_episodes”: Each call to sample() will return a batch of at most
# `rollout_fragment_length * num_envs_per_worker` in size. The batch will be exactly
# `rollout_fragment_length * num_envs` in size if postprocessing does not change batch
# sizes. Episodes may be truncated in order to meet this size requirement. This mode
# guarantees evenly sized batches, but increases variance as the future return must now
# be estimated at truncation boundaries.
# 2. “complete_episodes”: Each call to sample() will return a batch of at least
# `rollout_fragment_length * num_envs_per_worker` in size. Episodes will not be
# truncated, but multiple episodes may be packed within one batch to meet the (minimum)
# batch size. Note that when `num_envs_per_worker > 1`, episode steps will be buffered
# until the episode completes, and hence batches may contain significant amounts of
# off-policy data.
batch_mode: "truncate_episodes"

# Whether to validate that each created remote worker is healthy after its construction
# process.
validate_env_runners_after_construction: True

# Whether to attempt to continue training if a worker crashes. The number of currently
# healthy workers is reported as the “num_healthy_workers” metric.

# Whether - upon a worker failure - RLlib will try to recreate the lost worker as an
# identical copy of the failed one. The new worker will only differ from the failed one
# in its `self.recreated_worker=True` property value. It will have the same
# `worker_index` as the original one. If True, the `ignore_worker_failures` setting will
# be ignored.
# recreate_failed_workers: False

# If True and any sub-environment (within a vectorized env) throws any error during env
# stepping, the Sampler will try to restart the faulty sub-environment. This is done
# without disturbing the other (still intact) sub-environment and without the
# RolloutWorker crashing.

# Whether to LZ4 compress individual observations in the SampleBatches collected during
# rollouts.
compress_observations: False
